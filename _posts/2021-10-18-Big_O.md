---
title: What is Big-O Notation and Why Care?
layout: post
author: sjacobr
post-image: https://www.educative.io/cdn-cgi/image/f=auto,fit=contain,w=1200/api/page/5870871921033216/image/download/6036875259150336
description: Introduction to Big-O notation and how it works!
tags: 
- Big-O
- Computer Science
- Python
---

# Introduction

In this post, I intend to introduce Big-O notation and give other tips for optimizing your Python code. Writing code is not 
all about syntax and making sure it works. As data becomes large, efficiency becomes all the more important. And that is where Big-O comes in!

# Understanding Big-O

Big-O notation is a fundamental concept for all computer scientists, and it should be for data scientists too.

Big-O notation is the language we use for talking about how long an algorithm takes to run (time complexity) 
or how much memory is used by an algorithm (space complexity). 
Big-O notation can express the best, worst, and average-case running time of an algorithm. 
For our purposes here, we are going to focus primarily on Big-O as it relates to time complexity. 

You ask, “But Jake, why does Big-O matter?” Well, you might not care about code efficiency, but your boss probably will.
You could get by as a developer without understanding Big-O, but you might be costing your company time and money, which are the things
your company is trying to save by hiring you. This is _especially important_ if you are going to be working with big data.

Big-O discussions typically focus on the worst-case scenario (upper limit to how long an algorithm will take to run). 
An important piece to note is the “running time” when using Big-O notation does not directly equate to time as we know it 
(e.g., seconds, milliseconds, microseconds, etc.). We can think of running time as the number of operations or steps it takes to 
complete a problem of size n. In other words, Big-O notation is a way to track how quickly the runtime grows relative to the size of the input. 
When we are thinking about the worst-case, the question becomes — what are the most operations/steps that could happen for an input of size n?

Below I list a few common time complexities (from shortest time to longest) with an example to help grasp the concept. 

#### **O(1) → Constant Time**

O(1) means that it takes a constant time to run an algorithm, regardless of the size of the input.

Example: performing a math operation, popping/pushing a stack.

#### **O(log n) → Logarithmic Time**

O(log n) means that the running time grows in proportion to the logarithm of the input size, meaning that the run time barely 
increases as you exponentially increase the input. 

Example: Using a binary search tree. Say you searched the dictionary for a certain word, 
you can cut the dictionary in half and determine the word starts with a letter that is in one of the halves so you 
throw away the other half. You then cut the new dictionary in half again and decide which half the word would be in, throwing away the other half. 
Repeat this process until you find the word.

#### **O(n) → Linear Time**

O(n) means that the run time increases at the same pace as the input (more data means it takes more time to traverse).

Example: Usually a for loop indicates at least O(n) time complexity.

#### **O(n²) → Quadratic Time**

O(n²) means that the calculation runs in quadratic time, which is the squared size of the input data.

Example: A double for loop. As you can imagine, this is an awful time complexity. As your amount of data grows, your running time increases exponentially!

Big-O's visualized:

![](https://media-exp1.licdn.com/dms/image/C5612AQHxLM5ZfOQCMg/article-inline_image-shrink_1500_2232/0/1578938045559?e=1640822400&v=beta&t=Gteg-2ixllkyyQoFKZqr2IRQdaWGdJWZZrsPUSo4Y5I)

# Calculating Big-O

So how do you calculate your Big-O? Sometimes it is obvious (basic for loop), other times it is not. I will just go over the basics:

### Rule #1

Break your function into individual parts and calculate each part’s Big-O.

```python
def func():
  target_val = 26.17          # O(1)
  num = 0                     # O(1)
  for i in df.some_column:    # O(n)
    for j in i[0]:            # O(n) -> because it occurs within the first loop, multiply n * n = O(n²)
      if j == target_val      # O(1)
      num += 1                # O(1)
  
  lst = []
  for k in df.other_column:   # O(n)
    lst.append(k)             # O(1)
```

### Rule #2

Add up the Big-O of each operation together.

Based on function above:
O(1 + 1 + n² + 1 + 1 + n + 1) = O(5 + n² + n)

### Rule #3

Drop the constants. 

Based on above:
O(5 + n² + n) = O(n² + n)

### Rule #4

Drop non-dominant terms. If you have a double for loop, and elsewhere a single for loop, the Big-O becomes O(n²), not O(n² + n). 
This is because in the long-run, the n² will win out in terms of affecting the slope more.

Thus: O(n² + n) = O(n²)

As you write code, always ask yourself, “Based on what Jake taught me, what is my function’s Big-O complexity? Can I make it faster?” 
If you keep Big-O in mind, you will be a better, more thoughtful programmer. 
You will also be forced to understand the in's and out's of the code you are writing. And you just might impress that interviewer for that dream job!

# Python Tips 

Before I close, I just want to give a few simple tips.

### 1. Whenever you can, avoid for loops! Always vectorize if possible.

```python
my_list = []
start = time.time()
for i in df2['Closing Price']: # This
    m = i * 100                # Is
    my_list.append(m)          # Inefficient
    
df2['Closing Price'] = my_list
end = time.time()
for_loop_time = end - start
print("For loop time: " + str(for_loop_time))
# For loop time: 0.02755880355834961
```

Instead, if you use vectorization (operations that apply on entire data set at once), you will write faster code:

```python
start = time.time()
df3['Closing Price'] = df3['Closing Price'] * 100 # Yay!
end = time.time()
one_swoop_time = end - start
print("Vectorized time: " + str(one_swoop_time))
# Vectorized time: 0.004462718963623047
# Vectorizing this process is over 6 times faster!
```

I've seen vectorization be 80,000 times faster than using double for loops before. That is an insane amount of time saved!
With small datasets, it probably is not a huge issue. But if its data with millions of rows, you might be waiting hours.

### Did you know that dictionaries are faster than lists?

I see many use lists as their default, but this should not always be the case. Searching a list means you have to go through each
item until you find it (O(n)). Dictionaries can locate items in O(1) because of key-value pairs. However, if you have duplicate data,
you are out of luck. Keys in dictionaries cannot be repeated. Also, dictionaries are more memory-intensive than lists. So there are
trade-offs to be aware of.

### Do you need every column in your target dataset?

If you only want to inspect a few columns, pd.read_csv() has a 'usecols' argument that lets you do just that.
```python
cols = ['name', 'open', 'close']
df_partial = pd.read_csv("/Users/jake/Stat_426/big_five_stocks.csv", usecols = cols) # will contain 3 columns instead of 7
```
Less data = less time needed

# Conclusion
Big-O notation is critical to understanding your efficiency. I cannot go over every detail of it in this post, so I recommend doing further research
as you feel it necessary. Remember, always try to calculate your code's Big-O and ask yourself if you can do better!
